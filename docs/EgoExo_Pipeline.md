# Using GraVi-T with egoexo

Short project description goes here.

## Table of Contents

- [Setup](#Setup)
- [Run GraVi-T](#GraVi-T)
- [Run MLP](#MLP)


## Setup
1. Generate .bundle splits: `python /home/juro4948/gravit/data/egoexo4d/generate_splits.py` (not in GraVi-T repo)
2. Run util_scratchwork.ipynb to set up symlinks in /home/juro4948/gravit/GraVi-T/data/features. All samples in the train/validation groups should be placed in the corresponding split folder. The bundle files will inform the program which samples are in train vs. validation.


## Run GraVi-T
1. Generate the Pytorch-geometric graphs: 
    -   For aria single-view: `python data/generate_temporal_graphs.py --features egoexo-omnivore-aria --tauf 10 --dataset egoexo-omnivore-aria`
    - For GoPro:
        -    Multi-view: `python data/generate_temporal_graphs.py --features egoexo-omnivore-gopro --tauf 10 --dataset egoexo-omnivore-aria --is_multiview True`
    - At the moment, the code for egoexo-omnivore assumes that both the features and labels are at 30FPS. All EgoExo Omnivore features are at 30 FPS (even the GoPro streams) but the labels are generated by the preprocess.ipynb notebook. Currently I have it set to 30 FPS but if you are unsure then double check. 
    - The raw EgoExo GoPro data is at 60FPS, while the raw Aria data is at 30FPS.
2. Train model. `python tools/train_context_reasoning.py --cfg configs/action-segmentation/egoexo-omnivore/SPELL_default.yaml --split 2`

## Run MLP: 1st-person (Aria) features
1. Config file must contain this: 
    - graph_name: mlp
    - input_dim: 1536
    - dataset: egoexo-omnivore-aria
    - model_name: SimpleMLP

2. Train: `python tools/train_naive.py --cfg configs/action-segmentation/egoexo-omnivore/SPELL_default.yaml --split 1`
3. Evaluate: `python tools/evaluate_naive.py --dataset egoexo-omnivore-aria --exp_name SPELL_AS_default --eval_type AS`
